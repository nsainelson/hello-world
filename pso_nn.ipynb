{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMczLIHWNqk69XJI9Q6lqd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsainelson/hello-world/blob/master/pso_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-ApCTYaIuEF",
        "outputId": "907ce24d-5759-4675-922f-4ff5a8080ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 7500)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = np.round(np.random.rand(10000,9),3)\n",
        "X = data\n",
        "\n",
        "#print(data)\n",
        "#print(X)\n",
        "#print(X_train)\n",
        "\n",
        "y= np.random.choice([0,1], size=(10000))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)\n",
        "X_train = np.transpose(X_train)\n",
        "y_train = y_train.reshape(1,7500)\n",
        "X_test = np.transpose(X_test)\n",
        "#y_test = y_test.reshape(1,2500)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "#this function initialises the neural network by initialising the weight matrix and biases\n",
        "#def __init__(self,layer_dims):\n",
        "  parameters = {}\n",
        "  def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "  def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "  def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "  def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "  def linear_forward(A, W, b):\n",
        "\n",
        "    Z = np.dot(W, A) + b\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "  def linear_activation_forward(A_prev, W, b, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "      Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "      Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2    # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "      A_prev = A \n",
        "      A, cache = linear_activation_forward(A_prev, \n",
        "                                             parameters['W' + str(l)], \n",
        "                                             parameters['b' + str(l)], \n",
        "                                             activation='relu')\n",
        "      caches.append(cache)\n",
        "    \n",
        "    AL, cache = linear_activation_forward(A, \n",
        "                                          parameters['W' + str(L)], \n",
        "                                          parameters['b' + str(L)], \n",
        "                                          activation='sigmoid')\n",
        "    caches.append(cache)\n",
        "    assert(AL.shape == (1, X.shape[1]))\n",
        "            \n",
        "    return AL, caches\n",
        "def compute_cost(AL, Y):\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "    cost = (-1/m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL)))\n",
        "    \n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost\n",
        "def linear_backward(dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1 / m) * np.dot(dZ, cache[0].T)  \n",
        "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(cache[1].T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "def L_model_backward(AL, Y, caches):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "\n",
        "    dAL = dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    current_cache = caches[-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "        \n",
        "    return grads\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters['W' + str(l + 1)] - learning_rate * grads['dW' + str(l + 1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters['b' + str(l + 1)] - learning_rate * grads['db' + str(l + 1)]\n",
        "        \n",
        "    return parameters\n",
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    #print (\"predictions: \" + str(p))\n",
        "    #print (\"true labels: \" + str(y))\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "        \n",
        "    return p\n",
        "def print_mislabeled_images(classes, X, y, p):\n",
        "    \"\"\"\n",
        "    Plots images where predictions and truth were different.\n",
        "    X -- dataset\n",
        "    y -- true labels\n",
        "    p -- predictions\n",
        "    \"\"\"\n",
        "    a = p + y\n",
        "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
        "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
        "    num_images = len(mislabeled_indices[0])\n",
        "    for i in range(num_images):\n",
        "        index = mislabeled_indices[1][i]\n",
        "        \n",
        "        plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 1000, print_cost=False):#lr was 0.009\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []    # keep track of cost\n",
        "    \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "     \n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "               # Compute cost\n",
        "        cost = compute_cost(AL, Y)\n",
        "    \n",
        "        # Backward propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        " \n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "6Fxdje2fI6a1",
        "outputId": "49c9af19-4108-4172-d115-45aef0858d87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-1b48714aed55>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    }
  ]
}